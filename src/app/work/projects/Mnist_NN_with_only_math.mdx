---
title: "From Equations to Code: Building a Neural Network from Scratch"
publishedAt: "2025-05-07"
summary: "A detailed scientific report on designing and training a neural network from first principles using Python & NumPy, focusing on mathematical rationale and the practical lessons learned through implementation."
images:
  - "/images/projects/NN-from-scratch/nn_from_scratch.png"
  - "/images/projects/NN-from-scratch/learning_curve.png"
team:
  - name: "Ayoub Ben Chahla"
    role: "Software Engineer"
    avatar: "/images/avatar.jpg"
    linkedIn: "https://www.linkedin.com/in/ayoub-benchahla/"
link: "https://www.linkedin.com/in/ayoub-benchahla/"
---

## Overview

This project began as a personal challenge to understand neural networks at their most fundamental level. While modern frameworks like TensorFlow and PyTorch abstract away the underlying mathematics, I wanted to build everything from scratch using only Python and NumPy. This approach revealed the beautiful yet complex machinery that makes neural networks work - from the flow of derivatives during backpropagation to the careful initialization required for effective learning.

By implementing each component manually, I gained insights that are often hidden when using high-level APIs. This document walks through both the theoretical foundations and practical implementation details, with special attention to the "why" behind each decision.

## Mathematical Foundations

### Layer Computations

For any layer `$l$`:

`$ z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]} $`

`$ a^{[l]} = g(z^{[l]}) $`

**Deep Dive**:
- The `$ z^{[l]} $` term represents the "pre-activation" at layer `$ l $` - the weighted sum of inputs plus a bias term
- `$ W^{[l]} $` is the weight matrix that transforms inputs from layer `$ l-1 $` to layer `$ l $`
- `$ a^{[l-1]} $` contains the activated outputs from the previous layer
- `$ b^{[l]} $` is the bias vector that allows shifting the activation threshold
- `$ g() $` is the nonlinear activation function that enables the network to learn complex patterns

### Activation Functions

**ReLU (Rectified Linear Unit)**:
`$ g(z) = \max(0, z) $`

**Why ReLU?**
1. Avoids the vanishing gradient problem common in sigmoid/tanh
2. Computationally efficient (simple threshold operation)
3. Creates sparse activations which can lead to more efficient representations
4. Biological plausibility - resembles the firing rate of neurons

**Softmax (Output Layer)**:
`$ \sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}} $`

**Softmax Properties**:
- Converts raw scores (logits) into probabilities
- All outputs sum to 1.0
- Amplifies differences between scores (exponential effect)
- Used for multi-class classification problems

### Loss Function

Cross-entropy loss:
`$ L = -\frac{1}{m}\sum_{i=1}^m \sum_{k=1}^K y_k^{(i)}\log(\hat y_k^{(i)}) $`

**Components Explained**:
- `$ m $`: Number of training examples in the batch
- `$ K $`: Number of output classes
- `$ y_k^{(i)} $`: True label (1 if example `$ i $` belongs to class `$ k $`, else 0)
- `$ \hat y_k^{(i)} $`: Predicted probability for class `$ k $`
- The log term heavily penalizes confident wrong predictions

## Implementation Highlights

### Weight Initialization

```python
W = np.random.randn(output_dim, input_dim) * 0.01
b = np.zeros((output_dim, 1))
```

**Initialization Rationale**:
- Small random values (`* 0.01`) prevent early saturation of neurons
- Symmetry breaking: Different neurons start with different weights
- Zero initialization for biases is safe for ReLU networks
- For deeper networks, Xavier/Glorot initialization might be better

### Numerically Stable Softmax

```python
def softmax(z):
    z = z - np.max(z, axis=1, keepdims=True)
    exp_z = np.exp(z)
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)
```

**Stability Considerations**:
1. Subtracting `max(z)` prevents exponential overflow
2. `axis=1` operates across classes for each example
3. `keepdims=True` maintains 2D array structure
4. Normalization ensures probabilities sum to 1

### Complete Training Loop

```python
for epoch in range(epochs):
    # Forward pass
    Z1 = W1 @ X + b1.T  # Layer 1 pre-activation
    A1 = np.maximum(0, Z1)  # ReLU activation
    Z2 = W2 @ A1 + b2.T  # Output layer pre-activation
    A2 = softmax(Z2)  # Probability distribution
    
    # Backward pass
    dZ2 = A2 - Y  # Output error
    dW2 = (dZ2 @ A1.T) / m  # Gradient for W2
    db2 = np.sum(dZ2, axis=1, keepdims=True) / m  # Gradient for b2
    dA1 = W2.T @ dZ2  # Error backpropagated to A1
    dZ1 = dA1 * (Z1 > 0)  # ReLU derivative
    dW1 = (dZ1 @ X.T) / m  # Gradient for W1
    db1 = np.sum(dZ1, axis=1, keepdims=True) / m  # Gradient for b1
    
    # Parameter updates
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
```

**Key Implementation Details**:
- Matrix operations are fully vectorized for efficiency
- The `@` operator performs matrix multiplication
- ReLU gradient is implemented as `Z1 > 0` (indicator function)
- Gradients are averaged over the batch (divided by `m`)
- Parameters updated via simple gradient descent

## Experimental Setup

### Dataset: MNIST Handwritten Digits
- **60,000 training examples**, **10,000 test examples**
- Each image is **28×28 grayscale pixels** (784 features)
- Labels **one-hot encoded** into 10-dimensional vectors

### Network Architecture
```
Input (784) → Hidden Layer (128 neurons) → Output Layer (10 neurons)
```

### Hyperparameters


- Learning Rate   = 0.01    : Controls step size in weight updates 
- Batch Size      = 64      : Examples per gradient update 
- Epochs          = 20      : Complete passes through training data 
- Initialization  = Gaussian: (σ=0.01) | Small random starting weights 

## Results & Analysis

### Performance Metrics
- Training Accuracy = 94.1%  : How well model learned training patterns 
- Test Accuracy     = 92.7%  : Generalization to unseen data 
- Final Loss        = 0.19   : Cross-entropy loss value 

![Learning Curve](/images/projects/NN-from-scratch/learning_curve.png)

**Training Dynamics**:
- Rapid accuracy improvement in first 5 epochs
- Gradual convergence thereafter
- Small gap between training/test accuracy suggests good generalization

## Key Lessons Learned

### 1. Numerical Stability is Crucial
- **Softmax Overflow**: Without max subtraction, `exp(z)` can explode
- **Log Safety**: Added small epsilon (`1e-8`) to log arguments
- **Gradient Checks**: Verified derivatives via finite differences

### 2. Initialization Matters
- **Too Large**: Causes immediate saturation (all ReLUs dead)
- **Too Small**: Leads to tiny gradients and slow learning
- **Solution**: Found `σ=0.01` worked well for this architecture

### 3. Debugging Techniques
- **Dimension Checks**: Verified all matrix shapes at each step
- **Sanity Checks**: Confirmed loss decreases with small learning rate
- **Visualization**: Plotted histograms of activations/gradients

### 4. Computational Efficiency
- **Vectorization**: NumPy operations are ~100x faster than loops
- **Memory**: Pre-allocated matrices for intermediate values
- **Batching**: Parallel processing of multiple examples

## Conclusion

Building a neural network from scratch provided invaluable insights that framework users often miss. Key takeaways:

1. **Backpropagation** is fundamentally just the chain rule applied carefully through the computation graph
2. **Activation Functions** dramatically affect gradient flow and learning dynamics
3. **Numerical Stability** requires attention throughout implementation
4. **Hyperparameters** interact in complex ways that theory alone doesn't predict

This implementation, while not production-grade, captures the essence of deep learning systems. The knowledge gained forms a solid foundation for understanding more advanced architectures and optimization techniques. Future work could explore momentum, adaptive learning rates, and more sophisticated initialization schemes.

```
